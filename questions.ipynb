{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2 Q-Learning\n",
    "As our 1st algorithm, we use Q-Learning combined with $\\epsilon$-greedy policy – see section 6.5 of Sutton and Barto (2018) for details. At each time $t$, state $s_t$ is the board position (showing empty positions, positions taken by you, and positions taken by your opponent; c.f. tic_tac_toe.ipynb), action at is one of the available positions on the board (i.e. $\\epsilon$-greedy is applied only over the available actions), and reward $r_t$ is only non-zero when the game ends where you get $r_t$ = 1 if you win the game, $r_t$ = −1 if you lose, and $r_t$ = 0 if it is a draw.\n",
    "\n",
    "*Q*-Learning has 3 hyper-parameters: learning rate $\\alpha$, discount factor $\\gamma$, and exploration level $\\epsilon$. For\n",
    "convenience, we fix the learning rate at $\\alpha$ = 0.05 and the discount factor at γ = 0.99. We initialize all\n",
    "the *Q*-values at 0; if you are curious, you can explore the effect of $\\alpha$, $\\gamma$, and initial *Q*-values for yourself.\n",
    "\n",
    "### 2.1 Learning from experts\n",
    "\n",
    "In this section, you will study whether *Q*-learning can learn to play Tic Tac Toe by playing against Opt($\\epsilon_{opt}$) for some $\\epsilon_{opt}$ ∈ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm, run a Q-learning agent, with a fixed and arbitrary $\\epsilon$ ∈ [0, 1], against Opt(0.5) for 20’000 games – switch the 1st player after every game.\n",
    "### Question 1. \n",
    "Plot average reward for every 250 games during training – i.e. after the 50th game, plot the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to 100, etc. Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure of average reward over time (caption length < 50 words). Specify your choice of $\\epsilon$.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "### 2.1.1 Decreasing exploration\n",
    "One way to make training more efficient is to decrease the exploration level $\\epsilon$ over time. If we define $\\epsilon$(n)\n",
    "to be $\\epsilon$ for game number *n*, then one feasible way to decrease exploration during training is to use\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\epsilon(n) = max{{\\epsilon_{min}, \\epsilon_{max}(1 − \\frac{n}{n*})}}, (1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\epsilon_{min}$ and $\\epsilon_{max}$ are the minimum and maximum values for $\\epsilon$, respectively, and *n** is the number ofexploratory games and shows how fast $\\epsilon$ decreases. For convenience, we assume $\\epsilon_{min}$ = 0.1 and $\\epsilon_{max}$ = 0.8; if you are curious, you can explore their effect on performance for yourself. Use $\\epsilon$ as define above and run different *Q*-learning agents with different values of *n** against Opt(0.5) for 20’000 games – switch the 1st player after every game. Choose several values of *n** from a reasonably wide interval between 1 to 40’000 – particularly, include *n** = 1.\n",
    "Question 2. Plot average reward for every 250 games during training. Does decreasing \u000f help training\n",
    "compared to having a fixed \u000f? What is the effect of n\n",
    "∗\n",
    "?\n",
    "Expected answer: A figure showing average reward over time for different values of n\n",
    "∗\n",
    "(caption length <\n",
    "200 words).\n",
    "Question 3. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents\n",
    "– when measuring the ‘test’ performance, put \u000f = 0 and do not update the Q-values. Plot Mopt and\n",
    "Mrand over time. Describe the differences and the similarities between these curves and the ones of the\n",
    "previous question.\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of n\n",
    "∗\n",
    "(caption length\n",
    "< 100 words).\n",
    "2\n",
    "2.1.2 Good experts and bad experts\n",
    "Choose the best value of n\n",
    "∗\n",
    "that you found in the previous section. Run Q-learning against Opt(\u000fopt) for\n",
    "different values of \u000fopt for 20’000 games – switch the 1st player after every game. Choose several values\n",
    "of \u000fopt from a reasonably wide interval between 0 to 1 – particularly, include \u000fopt = 0.\n",
    "Question 4. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents\n",
    "– for each value of \u000fopt. Plot Mopt and Mrand over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of \u000fopt (caption length\n",
    "< 250 words).\n",
    "Question 5. What are the highest values of Mopt and Mrand that you could achieve after playing 20’000\n",
    "games?\n",
    "Question 6. (Theory) Assume that Agent 1 learns by playing against Opt(0) and find the optimal Q\u0002values Q1(s, a). In addition, assume that Agent 2 learns by playing against Opt(1) and find the optimal\n",
    "Q-values Q2(s, a). Do Q1(s, a) and Q2(s, a) have the same values? Justify your answer. (answer length\n",
    "< 150 words)\n",
    "2.2 Learning by self-practice\n",
    "In this section, your are supposed to ask whether Q-learning can learn to play Tic Tac Toe by only\n",
    "playing against itself. For different values of \u000f ∈ [0, 1), run a Q-learning agent against itself for 20’000\n",
    "games – i.e. both players use the same set of Q-values and update the same set of Q-values.\n",
    "Question 7. After every 250 games during training, compute the ‘test’ Mopt and Mrand for different\n",
    "values of \u000f ∈ [0, 1). Does the agent learn to play Tic Tac Toe? What is the effect of \u000f?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of \u000f ∈ [0, 1) (caption\n",
    "length < 100 words).\n",
    "For rest of this section, use \u000f(n) in Equation 1 with different values of n\n",
    "∗ – instead of fixing \u000f.\n",
    "Question 8. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents.\n",
    "Does decreasing \u000f help training compared to having a fixed \u000f? What is the effect of n\n",
    "∗\n",
    "?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of speeds of n\n",
    "∗\n",
    "(caption\n",
    "length < 100 words).\n",
    "Question 9. What are the highest values of Mopt and Mrand that you could achieve after playing 20’000\n",
    "games?\n",
    "Question 10. For three board arrangements (i.e. states s), visualize Q-values of available actions (e.g.\n",
    "using heat maps). Does the result make sense? Did the agent learn the game well?\n",
    "Expected answer: A figure with 3 subplots of 3 different states with Q-values shown at available actions\n",
    "(caption length < 200 words).\n",
    "3 Deep Q-Learning\n",
    "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with \u000f-greedy policy. You can watch\n",
    "again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of\n",
    "Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is\n",
    "to approximate Q-values by a neural network instead of a look-up table as in Tabular Q-learning. For\n",
    "implementation, you can use ideas from the DQN tutorials of Keras and PyTorch.\n",
    "3\n",
    "3.1 Implementation details\n",
    "State representation: We represent state st by a 3 × 3 × 2 tensor x t. Each element of x t takes\n",
    "a value of 0 or 1. The 3 × 3 matrix x t[:,:,0] shows positions taken by you, and x t[:,:,1] shows\n",
    "positions taken by your opponent. If x t[i,j,0] = x t[i,j,1] = 0, then position (i, j) is available.\n",
    "Neural network architecture: We use a fully connected network. State x t is fed to the network at\n",
    "the input layer. We consider 2 hidden layers each with 128 neurons – with ReLu activation functions.\n",
    "The output layer has 9 neurons (for 9 different actions) with linear activation functions. Each neuron at\n",
    "the output layer shows the Q-value of the corresponding action at state x t.\n",
    "Unavailable actions: For DQN, we do not constraint actions to only available actions. However,\n",
    "whenever the agent takes an unavailable action, we end the game and give the agent a negative reward\n",
    "of value runav = −1.\n",
    "Free parameters: DQN has many hyper parameters. For convenience, we fix the discount factor at\n",
    "γ = 0.99. We assume a buffer size of 10’000 and a batch size of 64. We update the target network every\n",
    "500 games. Instead of squared loss, we use the Huber loss (with δ = 1) with Adam optimizer (c.f. the\n",
    "DQN tutorials of Keras and PyTorch). You can fine tune the learning rate if needed, but we suggest\n",
    "5 × 10−4 as a starting point.\n",
    "Other options? There are tens of different ways to make training of deep networks more efficient. Do\n",
    "you feel like trying some and learning more? You are welcome to do so; you just need to explain the\n",
    "main features of your implementation and a brief summary of your reasoning in less than 300 words –\n",
    "under the title ‘Implementation details’ in your report.\n",
    "3.2 Learning from experts\n",
    "Implement the DQN algorithm. To check the algorithm, run a DQN agent with a fixed and arbitrary\n",
    "\u000f ∈ [0, 1) against Opt(0.5) for 20’000 games – switch the 1st player after every game.\n",
    "Question 11. Plot average reward and average training loss for every 250 games during training. Does\n",
    "the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure with two subplots (caption length < 50 words). Specify your choice of \u000f.\n",
    "Question 12. Repeat the training but without the replay buffer and with a batch size of 1: At every\n",
    "step, update the network by using only the latest transition. What do you observe?\n",
    "Expected answer: A figure with two subplots showing average reward and average training loss during\n",
    "training (caption length < 50 words).\n",
    "Instead of fixing \u000f, use \u000f(n) in Equation 1. For different values of n\n",
    "∗\n",
    ", run your DQN against Opt(0.5)\n",
    "for 20’000 games – switch the 1st player after every game. Choose several values of n\n",
    "∗\n",
    "from a reasonably\n",
    "wide interval between 1 to 40’000 – particularly, include n\n",
    "∗ = 1.\n",
    "Question 13. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents.\n",
    "Plot Mopt and Mrand over time. Does decreasing \u000f help training compared to having a fixed \u000f? What is\n",
    "the effect of n\n",
    "∗\n",
    "?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of speeds of n\n",
    "∗\n",
    "(caption\n",
    "length < 250 words).\n",
    "Choose the best value of n\n",
    "∗\n",
    "that you found. Run DQN against Opt(\u000fopt) for different values of \u000fopt for\n",
    "20’000 games – switch the 1st player after every game. Choose several values of \u000fopt from a reasonably\n",
    "wide interval between 0 to 1 – particularly, include \u000fopt = 0.\n",
    "Question 14. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents\n",
    "4\n",
    "– for each value of \u000fopt. Plot Mopt and Mrand over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of \u000fopt (caption length\n",
    "< 250 words).\n",
    "Question 15. What are the highest values of Mopt and Mrand that you could achieve after playing 20’000\n",
    "games?\n",
    "3.3 Learning by self-practice\n",
    "For different values of \u000f ∈ [0, 1), run a DQN agent against itself for 20’000 games – i.e. both players use\n",
    "the same neural network and share the same replay buffer. Important note: For one player, you should\n",
    "add states st and st\n",
    "0 as x t and x tp to the replay buffer, but for the other player, you should first swap\n",
    "the opponent positions (x t[:,:,1] and x tp[:,:,1]) with the agent’s own positions (x t[:,:,0] and\n",
    "x tp[:,:,0]) and then add them to the replay buffer.\n",
    "Question 16. After every 250 games during training, compute the ‘test’ Mopt and Mrand for different\n",
    "values of \u000f ∈ [0, 1). Plot Mopt and Mrand over time. Does the agent learn to play Tic Tac Toe? What is\n",
    "the effect of \u000f?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of \u000f ∈ [0, 1) (caption\n",
    "length < 100 words).\n",
    "Instead of fixing \u000f, use \u000f(n) in Equation 1 with different values of n\n",
    "∗\n",
    ".\n",
    "Question 17. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents.\n",
    "Plot Mopt and Mrand over time. Does decreasing \u000f help training compared to having a fixed \u000f? What is\n",
    "the effect of n\n",
    "∗\n",
    "?\n",
    "Expected answer: A figure showing Mopt and Mrand over time for different values of speeds of n\n",
    "∗\n",
    "(caption\n",
    "length < 100 words).\n",
    "Question 18. What are the highest values of Mopt and Mrand that you could achieve after playing 20’000\n",
    "games?\n",
    "Question 19. For three board arrangements (i.e. states s), visualize Q-values of available actions (e.g.\n",
    "using heat maps). Does the result make sense? Did the agent learn the game well?\n",
    "Expected answer: A figure with 3 subplots of 3 different states with Q-values shown at available actions\n",
    "(caption length < 200 words).\n",
    "4 Comparing Q-Learning with Deep Q-Learning\n",
    "We define the training time Ttrain as the number of games an algorithm needs to play in order to reach\n",
    "80% of its final performance according to both Mopt and Mrand.\n",
    "Question 20. Include a table showing the best performance (the highest Mopt and Mrand) of Q-Learning\n",
    "and DQN (both for learning from experts and for learning by self-practice) and their corresponding\n",
    "training time.\n",
    "Expected answer: A table showing 12 values.\n",
    "Question 21. Compare your results for DQN and Q-Learning (answer length < 300 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Players import *\n",
    "from runs import *\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import seaborn as sns\n",
    "from tic_env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2 Q-Learning\n",
    "As our 1st algorithm, we use Q-Learning combined with $\\epsilon$-greedy policy – see section 6.5 of Sutton and Barto (2018) for details. At each time $t$, state $s_t$ is the board position (showing empty positions, positions taken by you, and positions taken by your opponent; c.f. tic_tac_toe.ipynb), action at is one of the available positions on the board (i.e. $\\epsilon$-greedy is applied only over the available actions), and reward $r_t$ is only non-zero when the game ends where you get $r_t$ = 1 if you win the game, $r_t$ = −1 if you lose, and $r_t$ = 0 if it is a draw.\n",
    "\n",
    "*Q*-Learning has 3 hyper-parameters: learning rate $\\alpha$, discount factor $\\gamma$, and exploration level $\\epsilon$. For\n",
    "convenience, we fix the learning rate at $\\alpha$ = 0.05 and the discount factor at γ = 0.99. We initialize all\n",
    "the *Q*-values at 0; if you are curious, you can explore the effect of $\\alpha$, $\\gamma$, and initial *Q*-values for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Learning from experts\n",
    "\n",
    "In this section, you will study whether *Q*-learning can learn to play Tic Tac Toe by playing against Opt($\\epsilon_{opt}$) for some $\\epsilon_{opt}$ ∈ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm, run a Q-learning agent, with a fixed and arbitrary $\\epsilon$ ∈ [0, 1], against Opt(0.5) for 20’000 games – switch the 1st player after every game.\n",
    "### Question 1. \n",
    "Plot average reward for every 250 games during training – i.e. after the 50th game, plot the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to 100, etc. Does the agent learn to play Tic Tac Toe? \\\n",
    "*Expected answer:* A figure of average reward over time (caption length < 50 words). Specify your choice of $\\epsilon$.\n",
    "\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = [QLearningPlayer(eps=0.1), QLearningPlayer(eps=0.3), QLearningPlayer(eps=0.5), QLearningPlayer(eps=0.7), QLearningPlayer(eps=0.9)]\n",
    "env = TictactoeEnv()\n",
    "rewards = []\n",
    "for player in players:\n",
    "    reward, _, _ = run_against_Opt(player, n_games=20000, opt_eps=0.5)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "plt.figure(figsize = (10,7))\n",
    "epsilons = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "x = np.arange(0,20000,250)\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    plt.plot(x, rewards[i], label = r\"$\\epsilon = {:.2f}$\".format(epsilon))\n",
    "plt.xlabel('Number of games')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Q1. Learning from experts')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Decreasing exploration\n",
    "One way to make training more efficient is to decrease the exploration level $\\epsilon$ over time. If we define $\\epsilon$(n)\n",
    "to be $\\epsilon$ for game number *n*, then one feasible way to decrease exploration during training is to use\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\epsilon(n) = max({\\epsilon_{min}, \\epsilon_{max}(1 − \\frac{n}{n*})}), (1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\epsilon_{min}$ and $\\epsilon_{max}$ are the minimum and maximum values for $\\epsilon$, respectively, and *n** is the number of  exploratory games and shows how fast $\\epsilon$ decreases. For convenience, we assume $\\epsilon_{min}$ = 0.1 and $\\epsilon_{max}$ = 0.8; if you are curious, you can explore their effect on performance for yourself. Use $\\epsilon$ as define above and run different *Q*-learning agents with different values of *n** against Opt(0.5) for 20’000 games – switch the 1st player after every game. Choose several values of *n** from a reasonably wide interval between 1 to 40’000 – particularly, include *n** = 1.\n",
    "#### Question 2. \n",
    "Plot average reward for every 250 games during training. Does decreasing $\\epsilon$ help training\n",
    "compared to having a fixed $\\epsilon$? What is the effect of *n** ? \\\n",
    "*Expected answer:* A figure showing average reward over time for different values of *n** (caption length < 200 words).\n",
    "\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
